{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "\n",
    "Before selecting the model described bellow, different models and architectures were implemented.\n",
    "\n",
    "The different models trained, along with their performance scores are available on the Readme file.\n",
    "\n",
    "For space purpose, in this notebook, only the prefered model is presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.JPIJNSWNNAN3CE6LLI5FWSPHUT2VXMTH.gfortran-win_amd64.dll\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "C:\\conda\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\conda\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "#Preparation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Layers\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing bad canditates with tweets with length less than 3 and over 50\n",
    "def check_length(tweet):\n",
    "    length = len([word for word in tweet.split()])\n",
    "    if (length > 3) & (length < 50) :\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Dataset for Training**\n",
    "\n",
    "* In order to reduce the size of the Happy and sad class, tweets that are less than 3 or over 50 words are removed.\n",
    "* In addition, to keep the classes balanced, a sample of 15000 tweets is obtained from both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Tweet\n",
      "Class         \n",
      "Anger    11179\n",
      "Fear      9535\n",
      "Happy    14288\n",
      "Sadness  13803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#Will add all preprocessing here!\n",
    "#Scrapping the last set of tweets!\n",
    "df = pd.read_csv('D:/Uni_Stuff/Real-time twitter emotion detection/final1.csv')\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "#for happy:\n",
    "happy = df[df['Class'] == 'Happy']\n",
    "happy['Check'] = happy.apply(lambda row : check_length(row['Tweet']) , axis = 1)\n",
    "happy = happy[happy['Check'] == True].sample(15000)\n",
    "#For sadness\n",
    "sad = df[df['Class'] == 'Sadness']\n",
    "sad['Check'] = sad.apply(lambda row : check_length(row['Tweet']) , axis = 1)\n",
    "sad = sad[sad['Check'] == True].sample(15000) #Increasing the sample here\n",
    "\n",
    "merged = happy.append(sad)\n",
    "del merged['Check']\n",
    "\n",
    "\n",
    "final = df.loc[(df['Tweet'].isin(merged['Tweet']))|(df['Class']=='Anger')|(df['Class']=='Fear')].drop_duplicates(['Tweet'])\n",
    "classes = final.groupby('Class').count()\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizing\n",
    "* Padding\n",
    "* Encoding the labels\n",
    "* Splitting into train-test \n",
    "* Initializing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "#Lengths of max(for padding)\n",
    "max_length = np.max([len(tweet.split()) for tweet in final['Tweet']])\n",
    "\n",
    "#Tokenizing\n",
    "tk = Tokenizer(lower=True , split=\" \", filters='$%^*)(][_~}{:;\\/') #filtering out some panchuations more\n",
    "tk.fit_on_texts(final['Tweet'])\n",
    "total_words = len(tk.word_index) + 1 #total words\n",
    "#Converting\n",
    "tweet_seq = tk.texts_to_sequences(final['Tweet'])\n",
    "\n",
    "#padding\n",
    "tweet_length = max_length\n",
    "tweets_seq_pad = pad_sequences(tweet_seq , maxlen = tweet_length)\n",
    "\n",
    "#Encoding the labels\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(final['Class'])\n",
    "labels_cat = to_categorical(labels_encoded)\n",
    "\n",
    "#Spliting train-test\n",
    "xtrain , xtest , ytrain , ytest = train_test_split(tweets_seq_pad , labels_cat , test_size = 0.1 , random_state = 37)\n",
    "vocabulary = np.zeros((total_words , 200)) # <-creating the voc matrix, its of shape (total_words,200) where 200 is the dim of glove\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "As the Embedding Layer is extremely important for LSTM models , we will use the pre-trained GloVe layer, specialized for twitter's vocabulary.\n",
    "\n",
    "The 200-dimensions Layer was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the embedding layer\n",
    "\n",
    "print('Loading Vectors')\n",
    "print('...............')\n",
    "\n",
    "embeddings = dict()\n",
    "f = open('D:/Big Data/project/embeddings/glove.twitter.27B.200d.txt', 'r', encoding = \"utf-8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0] #first element is the word\n",
    "    #print(\"\\n\",word)\n",
    "    coefs = np.asarray(values[1:],dtype='float32') #all the other are the 200 coefs\n",
    "    embeddings[word] = coefs\n",
    "f.close()\n",
    "print('Loaded ', len(embeddings),\" words vectors'\")\n",
    "for word,i in tk.word_index.items(): #from this dictionary i am getting the words\n",
    "    word_vect = embeddings.get(word) # <- for this word i am getting each for the 200 values\n",
    "    if word_vect is not None: # <- if the word exists in the dictionary and its not None\n",
    "        vocabulary[i] = word_vect #<- i am adding it to the vocabulary,\n",
    "#Creating the Embeddings layer:\n",
    "embedding_layer = Embedding(input_dim = total_words, #My total words\n",
    "                            output_dim = 200 ,         #The output is the number of dimensions for each word\n",
    "                            weights = [vocabulary],    #The weights i have already defined\n",
    "                           input_length = tweet_length,  #The padded size\n",
    "                           trainable = False)  #freazing the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "This particular model was selected as it produce the best F1 and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(256,\n",
    "               dropout = 0.5, \n",
    "               recurrent_dropout = 0.5,\n",
    "               return_sequences = True)))\n",
    "#Second Layer\n",
    "model.add(Bidirectional(LSTM(128,\n",
    "               dropout = 0.5,\n",
    "               recurrent_dropout = 0.5 , return_sequences = False)))\n",
    "\n",
    "model.add(Dense(4,activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', \n",
    "               loss = 'categorical_crossentropy', \n",
    "               metrics = ['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xtrain, ytrain,\n",
    "                    validation_split = 0.15,\n",
    "                    epochs = 42,\n",
    "                        batch_size = 256,\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up  Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from elasticsearch import Elasticsearch\n",
    "import emoji\n",
    "import itertools \n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.dicts.noslang import slangdict\n",
    "from geopy import geocoders  \n",
    "from urllib3.exceptions import ProtocolError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweepy Set up\n",
    "#setting up the consumer's keys\n",
    "consumer_key = \"Twitter is very strict and doesnt let me share this\"\n",
    "consumer_secret = \"Twitter is very strict and doesnt let me share this\"\n",
    "access_token = \"Twitter is very strict and doesnt let me share this\"\n",
    "access_token_secret = \"Twitter is very strict and doesnt let me share this\"\n",
    "\n",
    "#setting up my authontication\n",
    "auth = tweepy.OAuthHandler(consumer_key , consumer_secret)\n",
    "auth.set_access_token(access_token , access_token_secret)\n",
    "api = tweepy.API(auth , wait_on_rate_limit = True)\n",
    "\n",
    "#Opening geopy connection to get the geolocation\n",
    "gn = geocoders.GeoNames(username = 'filtheo')\n",
    "\n",
    "#running bin/elasticsearch to have it ready\n",
    "#opening localhost:9200\n",
    "#Setting up elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline\n",
    "\n",
    "Preparing the same pipeline used to clean the tweets used for training.\n",
    "\n",
    "Before feeding a newly arrived tweet into the trained model, cleaning it the same way the training set was cleaned is vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up pre-processing functions\n",
    "pkl_file = open('extra1.p', 'rb') # => https://github.com/charlesmalafosse/FastText-sentiment-analysis-for-tweets/blob/master/betsentiment_sentiment_analysis_fasttext.py\n",
    "extra = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('slang.p', 'rb') # => http://pydoc.net/ekphrasis/0.4.7/ekphrasis.dicts.noslang.slangdict/\n",
    "slang = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "#Ekphprasis pipeline!\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize = ['url', 'email', 'percent', 'money', 'phone', 'user','time', 'url', 'date', 'number'],\n",
    "    \n",
    "     # terms that will be annotated =>flagged\n",
    "    #I can add \"hashtag\" here too => I removed it because i dont know what to do\n",
    "    annotate = {\"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored','hashtags'},\n",
    "    fix_html = True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used for word segmentation \n",
    "    segmenter = \"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be usedfor spell correction\n",
    "    corrector = \"twitter\", \n",
    "    \n",
    "    unpack_hashtags = True,  # perform word segmentation on hashtags <-removes the hashtag symbol an treats it as a word\n",
    "    unpack_contractions = True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong = False,  # spell correction for elongated words\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Tokenizes and then rejoins while getting rid of some terms\n",
    "    \n",
    "    #Set hashtags to true to keep hashtags \n",
    "    #I can set it to keep other stuff too:\n",
    "    #See documentation: https://github.com/cbaziotis/ekphrasis/blob/master/ekphrasis/classes/tokenizer.py\n",
    "    #On kwargs\n",
    "    tokenizer = SocialTokenizer(lowercase = True , hashtags = True , emojis = True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. =>slang is a dic created and saved as pickle\n",
    "    #documentation for dictionaries : http://pydoc.net/ekphrasis/0.4.7/ekphrasis.dicts.emoticons/\n",
    "    dicts = [emoticons]\n",
    ")\n",
    "\n",
    "#replace repeating <user> and <url>\n",
    "def repeated(tweet):\n",
    "    if ('<user>' not in tweet) & ('<url>' not in tweet):\n",
    "        return(tweet)\n",
    "    else:\n",
    "        cleaned_words = [word for word , zzzz in itertools.groupby(tweet.split())]\n",
    "        return(\" \".join(cleaned_words))\n",
    "\n",
    "#Get geolocation\n",
    "def get_geopoint(city):\n",
    "      try:\n",
    "        latitude = gn.geocode(city).latitude\n",
    "        longitude = gn.geocode(city).longitude\n",
    "        location = str(latitude)+\",\"+str(longitude)\n",
    "        return(location)\n",
    "      except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "def pre_process(tweet):\n",
    "  #Replacing panchuations and words\n",
    "    tweet = tweet.encode('latin1','ignore').decode('utf-8','ignore')\n",
    "    tweet = tweet.replace(\"’\",\"'\")#for some words\n",
    "    tweet = tweet.replace(\"‘\",\"'\") #for some words\n",
    "    tweet = tweet.replace('\"','')\n",
    "    tweet = tweet.replace(\"'\",\"\")\n",
    "    tweet = tweet.replace('\\n','..') #for newlines\n",
    "    tweet = tweet.replace('&','and') #reducing pancuations\n",
    "    tweet = tweet.replace(',','') #Gettind rid of commas\n",
    "\n",
    "    #Demojiing\n",
    "    tweet = emoji.demojize(tweet, delimiters=(\" \", \" \"))\n",
    "\n",
    "    #Stemming words from the dictionaries and replacing twitter Slang\n",
    "    reformed = [extra[word] if word in extra else word for word in tweet.split()]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #For slang:\n",
    "    reformed = [slang[word] if word in slang else word for word in tweet.split()]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #Ekprhasis pipeline\n",
    "    tweet = \" \".join(text_processor.pre_process_doc(tweet))\n",
    "    #replacing consecutive <urls>\n",
    "    #Adding it into an array so i can prepare it for my model\n",
    "    tweet = [repeated(tweet)]\n",
    "    #Tokenizing\n",
    "    tweet_in_seq = tk.texts_to_sequences(tweet)\n",
    "    #Padding\n",
    "    tweet_seq_padded = pad_sequences(tweet_in_seq , maxlen = tweet_length)\n",
    "    return(tweet_seq_padded)\n",
    "\n",
    "#Return the emotion\n",
    "def emotion_decoder(prediction):\n",
    "    value = np.argmax(prediction[0])\n",
    "    return(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing an ElasticSearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the mapping for the elastic search index\n",
    "settings = {                   \n",
    "    \"settings\": {                        #<---Basic settings,slide 25\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {                       #<--maping is equal to schema! slide 28\n",
    "        \"properties\": {                 #<- starting defying schema\n",
    "                \"Tweet\": {             #<--text\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"Location\": {               #<--for location\n",
    "                    \"type\": \"geo_point\"     #<- defying geo_point so i can add it to the map\n",
    "                },\n",
    "                \"Date\": {                  #<-- for date\n",
    "                    \"type\": \"date\"        #<- type date for line plot\n",
    "                },\n",
    "                \"Emotion\": {             #<-- predicted emotion\n",
    "                    \"type\": \"float\"     #<- had a problem with int so i use float\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "#Creating a new index here\n",
    "es.indices.create(index = 'final_final_test' , body = settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Pipeline\n",
    "\n",
    "* Reading a new tweet\n",
    "* Applying the pre-processing function to get the cleaned tweet\n",
    "* Feeding the cleaned tweet into the model and getting the prediction\n",
    "* Getting the geo-location of the author\n",
    "* Saving the cleaned tweet, the emotion , the geolocation and the date to the initialized index\n",
    "\n",
    "**In case an error occurs, the pipeline jumps to the next tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the Streaming\n",
    "class TweetStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    # on success\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "\n",
    "            # Getting The json\n",
    "            dict_data = json.loads(data)\n",
    "            #Getting the text\n",
    "            text = dict_data[\"text\"]\n",
    "            #Pre-process tweet\n",
    "            tweet = pre_process(text) #returns a sequance ready for the model\n",
    "            #pass tweet into my model\n",
    "            predicted = model.predict(tweet)\n",
    "            #Decode the predicted label\n",
    "            emotion = emotion_decoder(predicted).astype(float)\n",
    "            #Get the Date\n",
    "            date_created = str(dict_data['created_at'])\n",
    "            formatted_date = datetime.strptime(date_created, '%a %b %d %H:%M:%S %z %Y')\n",
    "            #Get geolocation\n",
    "            geo = get_geopoint(dict_data['user']['location'])\n",
    "            #Add data to elasticsearch index\n",
    "            es.index(index = 'final_final_test',\n",
    "                  body = {\"Tweet\": text,\n",
    "                        \"Location\": geo,\n",
    "                       \"Date\": formatted_date, #that must be date type\n",
    "                       \"Emotion\": emotion, #that must be number type\n",
    "                       })\n",
    "            print('Tweet Approved,Writting Index')\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        except:  #I got weird errors for specific tweets with different encoding\n",
    "            print('Error,nothing Written!')\n",
    "\n",
    "    # on failure\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "        return(True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # create instance of the tweepy tweet stream listener\n",
    "    listener = TweetStreamListener()\n",
    "\n",
    "   # create instance of the tweepy stream\n",
    "    stream = tweepy.Stream(auth, listener)\n",
    "    while True: \n",
    "        try:\n",
    "            stream.filter(track=['#coronavirus'],stall_warnings=True)   # search twitter for \"#coronavirus\"\n",
    "\n",
    "        except (ProtocolError, AttributeError): #for some connection issues\n",
    "            continue\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
