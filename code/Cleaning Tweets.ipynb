{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process\n",
    "\n",
    "Before using acquired tweets to train the selected models, pre-processing them is necessary.\n",
    "\n",
    "**Several Pre-processing steps took place to sufficiently clean every tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bout time!!!!! \\xe2\\x9c\\x8a\\xf0\\x9f\\x8f\\xbd\\xf...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not thrilled that the dog wanted to walk so ea...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get up and get your butt on the ride! #loveyou...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am surrounded by internalised homophobia.\\xf...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@peacexxanna your nightmare</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57288</th>\n",
       "      <td>hate is an acid that decays its own container....</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57289</th>\n",
       "      <td>making progress on the #yogaroom\\n\\n7 boxes of...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57290</th>\n",
       "      <td>i need this again\\xf0\\x9f\\xa5\\xba @euphoriahbo...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57291</th>\n",
       "      <td>always add a side of bacon.\\n\\ncommitment is a...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57292</th>\n",
       "      <td>in a world that yearns for positivity and happ...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57293 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet  Class\n",
       "0      bout time!!!!! \\xe2\\x9c\\x8a\\xf0\\x9f\\x8f\\xbd\\xf...  Happy\n",
       "1      not thrilled that the dog wanted to walk so ea...  Happy\n",
       "2      get up and get your butt on the ride! #loveyou...  Happy\n",
       "3      i am surrounded by internalised homophobia.\\xf...  Happy\n",
       "4                            @peacexxanna your nightmare   Fear\n",
       "...                                                  ...    ...\n",
       "57288  hate is an acid that decays its own container....  Happy\n",
       "57289  making progress on the #yogaroom\\n\\n7 boxes of...  Happy\n",
       "57290  i need this again\\xf0\\x9f\\xa5\\xba @euphoriahbo...  Happy\n",
       "57291  always add a side of bacon.\\n\\ncommitment is a...  Happy\n",
       "57292  in a world that yearns for positivity and happ...  Happy\n",
       "\n",
       "[57293 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools \n",
    "import emoji\n",
    "import pickle\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.dicts.noslang import slangdict\n",
    "\n",
    "\n",
    "df=pd.read_csv('df_before_preprocess1.csv',encoding='utf-8')\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Preprocessing step\n",
    "\n",
    "* Change encoding to take advantage of emoticons and emojis\n",
    " * Each emoji and emoticon is translated into a keyword. For example \" :) \" is translated to <smiley_face>\n",
    "* Replacing \" ’ \" with ' \n",
    "* Replacing newlines symbol\n",
    "* Replacing some extra symbols to reduce pancuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        bout time!!!!!  raised_fist_medium_skin_tone  ...\n",
       "1        not thrilled that the dog wanted to walk so ea...\n",
       "2        get up and get your butt on the ride! #loveyou...\n",
       "3        i am surrounded by internalised homophobia. up...\n",
       "4                              @peacexxanna your nightmare\n",
       "                               ...                        \n",
       "57288    hate is an acid that decays its own container....\n",
       "57289    making progress on the #yogaroom....7 boxes of...\n",
       "57290    i need this again pleading_face  @euphoriahbo ...\n",
       "57291    always add a side of bacon.....commitment is a...\n",
       "57292    in a world that yearns for positivity and happ...\n",
       "Name: Tweet, Length: 57293, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Replacing symbols\n",
    "df['Tweet'] = df['Tweet'].str.decode('unicode_escape').str.encode('latin1','ignore').str.decode('utf-8')\n",
    "df['Tweet'] = df['Tweet'].str.replace(\"’\" , \"'\")#for some words\n",
    "df['Tweet'] = df['Tweet'].str.replace(\"‘\" , \"'\") #for some words\n",
    "df['Tweet'] = df['Tweet'].str.replace('\"' , '')\n",
    "df['Tweet'] = df['Tweet'].str.replace(\"'\" , \"\")\n",
    "df['Tweet'] = df['Tweet'].str.replace('\\n' , '..') #for newlines\n",
    "df['Tweet'] = df['Tweet'].str.replace('&' , 'and') #reducing pancuations\n",
    "df['Tweet'] = df['Tweet'].str.replace(',' , '') #Gettind rid of commas\n",
    "\n",
    "\n",
    "#Replacing emojis:\n",
    "#delimiter stands for adding whitespace before and after the converted emoji\n",
    "df['Tweet'] = df.apply(lambda row : emoji.demojize(row['Tweet'] , delimiters=(\" \", \" \"))  ,axis = 1) \n",
    "df['Tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Preprocessing step\n",
    "\n",
    "* Converting contractions to their complete form. For example I've => I have, It's => It is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e91764d7b2d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpkl_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extra1.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# => https://github.com/charlesmalafosse/FastText-sentiment-analysis-for-tweets/blob/master/betsentiment_sentiment_analysis_fasttext.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mextra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpkl_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpkl_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'slang.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# => http://pydoc.net/ekphrasis/0.4.7/ekphrasis.dicts.noslang.slangdict/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "pkl_file = open('extra1.p', 'rb') # => https://github.com/charlesmalafosse/FastText-sentiment-analysis-for-tweets/blob/master/betsentiment_sentiment_analysis_fasttext.py\n",
    "extra = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open('slang.p', 'rb') # => http://pydoc.net/ekphrasis/0.4.7/ekphrasis.dicts.noslang.slangdict/\n",
    "slang = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "\n",
    "\n",
    "def check_1(tweet):\n",
    "    #For extra:\n",
    "    reformed = [extra[word] if word in extra else word for word in tweet.split()]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #For slang:\n",
    "    reformed = [slang[word] if word in slang else word for word in tweet.split()]\n",
    "    tweet = \" \".join(reformed)   \n",
    "    return(tweet)\n",
    "df['Tweet'] = df.apply(lambda row : check_1(row['Tweet']),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Preprocessing step\n",
    "\n",
    "\n",
    "**Ekprasis Pipeline**: (https://github.com/cbaziotis/ekphrasis)\n",
    "\n",
    "* Normalize values such as mails , urls ,dates since they are irrelenant(we only need their type)\n",
    "* Annotate some values (didnt applied it on hashtags) \n",
    "* Fixed some Html values that might have escaped before\n",
    "* Segmented some words: for example retrogaming => retro gaming (based on Twitter vocabulary)\n",
    "* Corrected the spelling of some words (Based on twitter)\n",
    "* Didnt perform segmentation on hashtags \n",
    "* Unpacked some extra words such as cant't => can not\n",
    "* Tokenized and then rejoined to perform the operations\n",
    "* Used dictionaries to replace words after tokenizing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "#Ekphprasis pipeline!\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize = ['url', 'email', 'percent', 'money', 'phone', 'user','time', 'url', 'date', 'number'],\n",
    "    \n",
    "    # terms that will be annotated =>flagged\n",
    "    annotate = {\"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored','hashtags'},\n",
    "    fix_html = True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used for word segmentation \n",
    "    segmenter = \"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used for spell correction\n",
    "    corrector = \"twitter\", \n",
    "    \n",
    "    unpack_hashtags = True,  # perform word segmentation on hashtags <-removes the hashtag symbol and treats it as a word\n",
    "    unpack_contractions = True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong = False,  # spell correction for elongated words\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Tokenizes and then rejoins while getting rid of some terms\n",
    "    \n",
    "    #Set hashtags to true to keep hashtags \n",
    "    #I can set it to keep other stuff too:\n",
    "    #See documentation: https://github.com/cbaziotis/ekphrasis/blob/master/ekphrasis/classes/tokenizer.py\n",
    "    #On kwargs\n",
    "    tokenizer = SocialTokenizer(lowercase = True , hashtags = True , emojis = True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. =>slang is a dic created and saved as pickle\n",
    "    #documentation for dictionaries : http://pydoc.net/ekphrasis/0.4.7/ekphrasis.dicts.emoticons/\n",
    "    dicts = [emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the pipeline\n",
    "df['Tweet'] = df.apply(lambda row : \" \".join(text_processor.pre_process_doc(row['Tweet'])) , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some extra steps:\n",
    "* Replace repeated < user > tags or < url > with a single keyword\n",
    "* For example < user >,< user >,< user >,< user > => < user >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace repeating <user> and <url>\n",
    "#Example <user> <user> <user> <user> -> <user>\n",
    "def repeated(tweet):\n",
    "    if ('<user>' not in tweet) & ('<url>' not in tweet):\n",
    "        return(tweet)\n",
    "    else:\n",
    "        cleaned_words = [word for word,zzzz in itertools.groupby(tweet.split())]\n",
    "        return(\" \".join(cleaned_words))\n",
    "df['Tweet'] = df.apply(lambda row : (repeated(row['Tweet'])),axis=1)\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/Big Data/project/final sets/final1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
